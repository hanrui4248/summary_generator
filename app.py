import streamlit as st
import os
import base64
import datetime as dt
from arxiv_pdf import fetch_papers
from paper_affiliation_classifier import PaperAffiliationClassifier
from paper_assistant import PaperAssistant
import affiliation_analyzer
from abstract_matcher import AbstractMatcher
import re
import pandas as pd
from orgs import orgs
from docx import Document
from docx.shared import Inches
import io
import shutil


def get_download_link(content, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    href = f'<a href="data:text/markdown;base64,{b64}" download="{filename}">{text}</a>'
    return href

def get_binary_file_downloader_html(bin_file, file_label='æ–‡ä»¶', file_name='æ–‡ä»¶.docx'):
    """ç”ŸæˆäºŒè¿›åˆ¶æ–‡ä»¶ä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(bin_file).decode()
    href = f'<a href="data:application/octet-stream;base64,{b64}" download="{file_name}">{file_label}</a>'
    return href

def display_image(image_path, base_dir):
    """æ˜¾ç¤ºå›¾ç‰‡ï¼Œæ”¯æŒå¤šç§è·¯å¾„å°è¯•"""
    # å¦‚æœæ˜¯URLï¼Œç›´æ¥æ˜¾ç¤º
    if image_path.startswith(('http://', 'https://')):
        try:
            st.image(image_path)
            return True
        except Exception as e:
            st.warning(f"æ— æ³•åŠ è½½å›¾ç‰‡URL: {image_path}ï¼Œé”™è¯¯: {str(e)}")
            return False
    
    # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œå°è¯•å¤šç§è·¯å¾„ç»„åˆ
    possible_paths = [
        image_path,  # åŸå§‹è·¯å¾„
        os.path.join(base_dir, image_path),  # ç›¸å¯¹äºåŸºç¡€ç›®å½•
        os.path.abspath(image_path),  # ç»å¯¹è·¯å¾„
        os.path.join(os.getcwd(), image_path)  # ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            try:
                st.image(path)
                return True
            except Exception as e:
                st.warning(f"å›¾ç‰‡å­˜åœ¨ä½†æ— æ³•æ˜¾ç¤º: {path}ï¼Œé”™è¯¯: {str(e)}")
                break
    
    # st.warning(f"æ— æ³•æ‰¾åˆ°å›¾ç‰‡: {image_path}")
    return False

def process_markdown_line(line, base_dir):
    """å¤„ç†æ™®é€šçš„Markdownè¡Œï¼Œæ˜¾ç¤ºæ–‡æœ¬å’Œå›¾ç‰‡"""
    # æŸ¥æ‰¾è¡Œä¸­çš„å›¾ç‰‡æ ‡è®°
    img_pattern = r'!\[(.*?)\]\((.*?)\)'
    img_matches = re.findall(img_pattern, line)
    
    if img_matches:
        # æœ‰å›¾ç‰‡ï¼Œåˆ†å‰²æ–‡æœ¬å’Œå›¾ç‰‡
        parts = re.split(img_pattern, line)
        for i in range(len(parts)):
            if i % 3 == 0:  # æ–‡æœ¬éƒ¨åˆ†
                if parts[i].strip():
                    st.markdown(parts[i])
            elif i % 3 == 2:  # å›¾ç‰‡è·¯å¾„
                display_image(parts[i], base_dir)
    else:
        # æ²¡æœ‰å›¾ç‰‡ï¼Œç›´æ¥æ˜¾ç¤ºæ–‡æœ¬
        st.markdown(line)

def process_markdown_table(table_lines, base_dir):
    """å¤„ç†Markdownè¡¨æ ¼ï¼Œæ­£ç¡®æ˜¾ç¤ºè¡¨æ ¼ç»“æ„å’Œå›¾ç‰‡"""
    # è§£æè¡¨æ ¼ç»“æ„
    rows = []
    for line in table_lines:
        # è·³è¿‡åˆ†éš”è¡Œ (| --- | --- |)
        if re.match(r'\|\s*[-:]+\s*\|', line):
            continue
        
        # åˆ†å‰²å•å…ƒæ ¼
        cells = line.split('|')[1:-1]  # å»æ‰é¦–å°¾çš„ |
        processed_cells = []
        
        for cell in cells:
            # æ£€æŸ¥å•å…ƒæ ¼æ˜¯å¦åŒ…å«å›¾ç‰‡
            img_match = re.search(r'!\[(.*?)\]\((.*?)\)', cell)
            if img_match:
                # è¿™æ˜¯ä¸€ä¸ªåŒ…å«å›¾ç‰‡çš„å•å…ƒæ ¼ï¼Œä½¿ç”¨å›¾ç‰‡è·¯å¾„ä½œä¸ºå ä½ç¬¦
                img_path = img_match.group(2)
                processed_cells.append(img_path)
            else:
                # æ™®é€šæ–‡æœ¬å•å…ƒæ ¼
                processed_cells.append(cell.strip())
        
        rows.append(processed_cells)
    
    # åˆ›å»ºè¡¨æ ¼
    if rows:
        # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrame
        df = pd.DataFrame(rows)
        
        # ä½¿ç”¨st.columnsåˆ›å»ºè¡¨æ ¼å¸ƒå±€
        cols = st.columns(len(rows[0]))
        
        # å¡«å……è¡¨æ ¼å†…å®¹
        for col_idx, col in enumerate(cols):
            for row_idx in range(len(rows)):
                cell_content = rows[row_idx][col_idx]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡è·¯å¾„
                if re.match(r'.*\.(png|jpg|jpeg|gif|bmp|webp)', cell_content, re.IGNORECASE):
                    with col:
                        display_image(cell_content, base_dir)
                else:
                    with col:
                        st.markdown(cell_content)

def display_markdown_with_images(markdown_content, base_dir="."):
    """è§£æMarkdownå†…å®¹å¹¶ä½¿ç”¨Streamlitç»„ä»¶æ˜¾ç¤ºï¼ŒåŒ…æ‹¬å›¾ç‰‡å’Œè¡¨æ ¼"""
    # åˆ†å‰²å†…å®¹ä¸ºè¡Œ
    lines = markdown_content.split('\n')
    i = 0
    while i < len(lines):
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼å¼€å§‹
        if lines[i].strip().startswith('|') and '|' in lines[i][1:]:
            # æ”¶é›†è¡¨æ ¼è¡Œ
            table_lines = []
            while i < len(lines) and lines[i].strip().startswith('|'):
                table_lines.append(lines[i])
                i += 1
            
            # å¤„ç†è¡¨æ ¼
            process_markdown_table(table_lines, base_dir)
        else:
            # å¤„ç†æ™®é€šæ–‡æœ¬è¡Œ
            if lines[i].strip():
                process_markdown_line(lines[i], base_dir)
            i += 1

def markdown_to_docx(markdown_content, base_dir="."):
    """å°†Markdownå†…å®¹è½¬æ¢ä¸ºWordæ–‡æ¡£"""
    doc = Document()
    
    # æ·»åŠ æ ‡é¢˜
    doc.add_heading('æ¯æ—¥arXivè®ºæ–‡å¿«æŠ¥', 0)
    
    # åˆ†å‰²å†…å®¹ä¸ºè¡Œ
    lines = markdown_content.split('\n')
    i = 0
    
    # å½“å‰æ®µè½
    current_paragraph = None
    
    while i < len(lines):
        line = lines[i].strip()
        
        # å¤„ç†æ ‡é¢˜
        if line.startswith('# '):
            doc.add_heading(line[2:], 1)
            i += 1
        elif line.startswith('## '):
            doc.add_heading(line[3:], 2)
            i += 1
        elif line.startswith('### '):
            doc.add_heading(line[4:], 3)
            i += 1
        # å¤„ç†è¡¨æ ¼
        elif line.startswith('|') and '|' in line[1:]:
            # æ”¶é›†è¡¨æ ¼è¡Œ
            table_lines = []
            while i < len(lines) and lines[i].strip().startswith('|'):
                table_lines.append(lines[i].strip())
                i += 1
            
            # è§£æè¡¨æ ¼
            rows = []
            for table_line in table_lines:
                # è·³è¿‡åˆ†éš”è¡Œ
                if re.match(r'\|\s*[-:]+\s*\|', table_line):
                    continue
                
                # åˆ†å‰²å•å…ƒæ ¼
                cells = table_line.split('|')[1:-1]  # å»æ‰é¦–å°¾çš„ |
                rows.append([cell.strip() for cell in cells])
            
            if rows:
                # åˆ›å»ºè¡¨æ ¼
                table = doc.add_table(rows=len(rows), cols=len(rows[0]))
                
                # å¡«å……è¡¨æ ¼å†…å®¹
                for row_idx, row in enumerate(rows):
                    for col_idx, cell_content in enumerate(row):
                        # æ£€æŸ¥å•å…ƒæ ¼æ˜¯å¦åŒ…å«å›¾ç‰‡
                        img_match = re.search(r'!\[(.*?)\]\((.*?)\)', cell_content)
                        if img_match:
                            # è¿™æ˜¯ä¸€ä¸ªåŒ…å«å›¾ç‰‡çš„å•å…ƒæ ¼
                            img_path = img_match.group(2)
                            cell = table.cell(row_idx, col_idx)
                            
                            # å°è¯•æ·»åŠ å›¾ç‰‡
                            try:
                                possible_paths = [
                                    img_path,
                                    os.path.join(base_dir, img_path),
                                    os.path.abspath(img_path),
                                    os.path.join(os.getcwd(), img_path)
                                ]
                                
                                for path in possible_paths:
                                    if os.path.exists(path):
                                        cell.paragraphs[0].add_run().add_picture(path, width=Inches(2.0))
                                        break
                            except Exception as e:
                                cell.text = f"[å›¾ç‰‡: {img_path}]"
                        else:
                            # æ™®é€šæ–‡æœ¬å•å…ƒæ ¼
                            table.cell(row_idx, col_idx).text = cell_content
        
        # å¤„ç†å›¾ç‰‡
        elif '![' in line and '](' in line:
            img_pattern = r'!\[(.*?)\]\((.*?)\)'
            text_parts = re.split(img_pattern, line)
            
            # åˆ›å»ºæ–°æ®µè½
            current_paragraph = doc.add_paragraph()
            
            for i_part, part in enumerate(text_parts):
                if i_part % 3 == 0 and part.strip():  # æ–‡æœ¬éƒ¨åˆ†
                    current_paragraph.add_run(part)
                elif i_part % 3 == 2:  # å›¾ç‰‡è·¯å¾„
                    img_path = part
                    try:
                        possible_paths = [
                            img_path,
                            os.path.join(base_dir, img_path),
                            os.path.abspath(img_path),
                            os.path.join(os.getcwd(), img_path)
                        ]
                        
                        for path in possible_paths:
                            if os.path.exists(path):
                                current_paragraph.add_run().add_picture(path, width=Inches(4.0))
                                break
                    except Exception as e:
                        current_paragraph.add_run(f"[å›¾ç‰‡: {img_path}]")
            
            i += 1
        
        # å¤„ç†æ™®é€šæ–‡æœ¬
        elif line:
            doc.add_paragraph(line)
            i += 1
        else:
            # ç©ºè¡Œ
            i += 1
    
    # ä¿å­˜åˆ°å†…å­˜ä¸­
    docx_io = io.BytesIO()
    doc.save(docx_io)
    docx_io.seek(0)
    
    return docx_io.getvalue()

def main():
    # æ¸…ç©ºpdf_folderå’Œimagesæ–‡ä»¶å¤¹
    pdf_folder = "pdf_folder"
    if os.path.exists(pdf_folder):
        for file in os.listdir(pdf_folder):
            file_path = os.path.join(pdf_folder, file)
            if os.path.isfile(file_path):
                os.remove(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
    else:
        os.makedirs(pdf_folder)
    
    # æ¸…ç©ºimagesæ–‡ä»¶å¤¹
    images_folder = "images"
    if os.path.exists(images_folder):
        for file in os.listdir(images_folder):
            file_path = os.path.join(images_folder, file)
            if os.path.isfile(file_path):
                os.remove(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
    else:
        os.makedirs(images_folder)
    
    st.set_page_config(page_title="æ¯æ—¥arXivè®ºæ–‡å¿«æŠ¥", page_icon="ğŸ“š")
    
    st.title("AI4Paper")
    st.write("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ç”Ÿæˆä»Šæ—¥ç²¾é€‰è®ºæ–‡å¿«æŠ¥")
    
    # æ·»åŠ ä¸€äº›é…ç½®é€‰é¡¹
    with st.expander("é«˜çº§é…ç½®"):
        days_back = st.slider("æŸ¥è¯¢è¿‡å»å‡ å¤©çš„è®ºæ–‡", 1, 7, 1)
        
        # ç›®æ ‡æœºæ„å¤šé€‰
        default_orgs = orgs
        target_orgs = st.multiselect(
            "ç›®æ ‡æœºæ„", 
            options=default_orgs,
            default=default_orgs
        )
        
        # æ·»åŠ å…³é”®è¯æŸ¥è¯¢é€‰é¡¹
        use_keyword_filter = st.checkbox("ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿‡æ»¤è®ºæ–‡", value=False)
        keyword_query = ""
        if use_keyword_filter:
            keyword_query = st.text_input("è¯·è¾“å…¥æç¤ºè¯", placeholder="ä¾‹å¦‚ï¼šå¼ºåŒ–å­¦ä¹ ã€æœºå™¨äººã€å¤§æ¨¡å‹ç­‰")
    
    # ç”ŸæˆæŒ‰é’®
    if st.button("ç”Ÿæˆæ¯æ—¥è®ºæ–‡å¿«æŠ¥", type="primary"):
        with st.spinner("æ­£åœ¨è·å–å’Œå¤„ç†è®ºæ–‡ï¼Œè¯·ç¨å€™..."):
            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = st.progress(0)
            
            # ç¬¬ä¸€æ­¥ï¼šè¿è¡Œpipelineè·å–è®ºæ–‡,å¹¶æ‰“ä¸Šåˆ†ç±»æ ‡ç­¾
            st.info("æ­¥éª¤1/4: ä»arXivè·å–è®ºæ–‡...")
            progress_bar.progress(10)
            
            csv_filename = "papers.csv"
            pdf_folder = "pdf_folder"
            
            # ä½¿ç”¨é»˜è®¤çš„æŸ¥è¯¢å­—ç¬¦ä¸² "cat:cs.AI"
            query = "cat:cs.AI"
            
            end_date = dt.datetime.today()
            start_date = end_date - dt.timedelta(days=days_back)
            papers_count = fetch_papers(
                pdf_folder, 
                csv_filename=csv_filename,
                query=query,
                author_filter=False,
                start_date=start_date,
                end_date=end_date
            )
            
            if papers_count == 0:
                st.error("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œè¯·è°ƒæ•´æŸ¥è¯¢å‚æ•°åé‡è¯•ã€‚")
                return
            
            classifier = PaperAffiliationClassifier()
            classifier.process_csv(csv_filename)
            
            progress_bar.progress(30)
            
            # ç¬¬äºŒæ­¥ï¼šè·å–ç›®æ ‡æœºæ„çš„è®ºæ–‡ç´¢å¼•
            indices_result = []
            if target_orgs:
                st.info("æ­¥éª¤2/4: æ¨¡å‹ç­›é€‰ç›®æ ‡æœºæ„è®ºæ–‡...")
                analyzer = affiliation_analyzer.AffiliationAnalyzer()
                indices_result = analyzer.process_csv(csv_filename, target_orgs)
                progress_bar.progress(50)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®å…³é”®è¯è¿‡æ»¤è®ºæ–‡
            keyword_indices = []
            if use_keyword_filter and keyword_query:
                st.info("æ­¥éª¤3/4: æ ¹æ®å…³é”®è¯è¿‡æ»¤è®ºæ–‡...")
                matcher = AbstractMatcher()
                keyword_indices = matcher.process_csv(csv_filename, keyword_query)
                progress_bar.progress(70)
            else:
                st.info("æ­¥éª¤3/4: è·³è¿‡å…³é”®è¯è¿‡æ»¤...")
                progress_bar.progress(70)
            
            # åˆå¹¶è¿‡æ»¤ç»“æœ
            final_indices = []
            if target_orgs and use_keyword_filter and keyword_query:
                # ä¸¤ç§è¿‡æ»¤å™¨éƒ½å¯ç”¨ï¼Œå–äº¤é›†
                final_indices = list(set(indices_result).intersection(set(keyword_indices)))
                if not final_indices:
                    st.warning("æ²¡æœ‰åŒæ—¶æ»¡è¶³æœºæ„å’Œå…³é”®è¯æ¡ä»¶çš„è®ºæ–‡ï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰æ»¡è¶³æœºæ„æ¡ä»¶çš„è®ºæ–‡")
                    final_indices = indices_result
            elif target_orgs:
                # åªä½¿ç”¨æœºæ„è¿‡æ»¤
                final_indices = indices_result
            elif use_keyword_filter and keyword_query:
                # åªä½¿ç”¨å…³é”®è¯è¿‡æ»¤
                final_indices = keyword_indices
            else:
                # éƒ½ä¸ä½¿ç”¨ï¼Œè·å–æ‰€æœ‰è®ºæ–‡ç´¢å¼•
                df = pd.read_csv(csv_filename)
                final_indices = list(range(len(df)))
            
            if not final_indices:
                st.error("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œè¯·è°ƒæ•´è¿‡æ»¤æ¡ä»¶åé‡è¯•ã€‚")
                return
            
            # ç¬¬å››æ­¥ï¼šä¸‹è½½è®ºæ–‡å¹¶ç”Ÿæˆæ‘˜è¦
            st.info("æ­¥éª¤4/4: ç”Ÿæˆè®ºæ–‡æ‘˜è¦...")
            assistant = PaperAssistant(output_dir=pdf_folder)
            markdown_content = assistant.process_and_download(csv_filename, final_indices)
            
            progress_bar.progress(100)
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"æˆåŠŸç”Ÿæˆè®ºæ–‡å¿«æŠ¥ï¼Œå…±åŒ…å« {len(final_indices)} ç¯‡è®ºæ–‡ï¼ˆä» {papers_count} ç¯‡è®ºæ–‡ä¸­ç­›é€‰ï¼‰")
            # ä½¿ç”¨æ–°çš„display_markdown_with_imageså‡½æ•°æ˜¾ç¤ºmarkdownå†…å®¹
            display_markdown_with_images(markdown_content, pdf_folder)
            
            # ç”ŸæˆWordæ–‡æ¡£
            docx_content = markdown_to_docx(markdown_content, pdf_folder)
            
            # æä¾›ä¸‹è½½é“¾æ¥
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(
                    get_download_link(markdown_content, "AIç”Ÿæˆ_æ¯æ—¥arXivç²¾é€‰è®ºæ–‡.md", "ç‚¹å‡»ä¸‹è½½Markdownæ–‡ä»¶"),
                    unsafe_allow_html=True
                )
            with col2:
                st.markdown(
                    get_binary_file_downloader_html(docx_content, "ç‚¹å‡»ä¸‹è½½Wordæ–‡ä»¶", "AIç”Ÿæˆ_æ¯æ—¥arXivç²¾é€‰è®ºæ–‡.md.docx"),
                    unsafe_allow_html=True
                )
            

if __name__ == "__main__":
    main() 