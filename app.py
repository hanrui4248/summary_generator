import streamlit as st
import os
import base64
from paper_pipeline import run_pipeline
from paper_assistant import PaperAssistant
import affiliation_analyzer
import re
import pandas as pd

def get_download_link(content, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    href = f'<a href="data:text/markdown;base64,{b64}" download="{filename}">{text}</a>'
    return href

def display_image(image_path, base_dir):
    """æ˜¾ç¤ºå›¾ç‰‡ï¼Œæ”¯æŒå¤šç§è·¯å¾„å°è¯•"""
    # å¦‚æœæ˜¯URLï¼Œç›´æ¥æ˜¾ç¤º
    if image_path.startswith(('http://', 'https://')):
        try:
            st.image(image_path)
            return True
        except Exception as e:
            st.warning(f"æ— æ³•åŠ è½½å›¾ç‰‡URL: {image_path}ï¼Œé”™è¯¯: {str(e)}")
            return False
    
    # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œå°è¯•å¤šç§è·¯å¾„ç»„åˆ
    possible_paths = [
        image_path,  # åŸå§‹è·¯å¾„
        os.path.join(base_dir, image_path),  # ç›¸å¯¹äºåŸºç¡€ç›®å½•
        os.path.abspath(image_path),  # ç»å¯¹è·¯å¾„
        os.path.join(os.getcwd(), image_path)  # ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            try:
                st.image(path)
                return True
            except Exception as e:
                st.warning(f"å›¾ç‰‡å­˜åœ¨ä½†æ— æ³•æ˜¾ç¤º: {path}ï¼Œé”™è¯¯: {str(e)}")
                break
    
    # st.warning(f"æ— æ³•æ‰¾åˆ°å›¾ç‰‡: {image_path}")
    return False

def process_markdown_line(line, base_dir):
    """å¤„ç†æ™®é€šçš„Markdownè¡Œï¼Œæ˜¾ç¤ºæ–‡æœ¬å’Œå›¾ç‰‡"""
    # æŸ¥æ‰¾è¡Œä¸­çš„å›¾ç‰‡æ ‡è®°
    img_pattern = r'!\[(.*?)\]\((.*?)\)'
    img_matches = re.findall(img_pattern, line)
    
    if img_matches:
        # æœ‰å›¾ç‰‡ï¼Œåˆ†å‰²æ–‡æœ¬å’Œå›¾ç‰‡
        parts = re.split(img_pattern, line)
        for i in range(len(parts)):
            if i % 3 == 0:  # æ–‡æœ¬éƒ¨åˆ†
                if parts[i].strip():
                    st.markdown(parts[i])
            elif i % 3 == 2:  # å›¾ç‰‡è·¯å¾„
                display_image(parts[i], base_dir)
    else:
        # æ²¡æœ‰å›¾ç‰‡ï¼Œç›´æ¥æ˜¾ç¤ºæ–‡æœ¬
        st.markdown(line)

def process_markdown_table(table_lines, base_dir):
    """å¤„ç†Markdownè¡¨æ ¼ï¼Œæ­£ç¡®æ˜¾ç¤ºè¡¨æ ¼ç»“æ„å’Œå›¾ç‰‡"""
    # è§£æè¡¨æ ¼ç»“æ„
    rows = []
    for line in table_lines:
        # è·³è¿‡åˆ†éš”è¡Œ (| --- | --- |)
        if re.match(r'\|\s*[-:]+\s*\|', line):
            continue
        
        # åˆ†å‰²å•å…ƒæ ¼
        cells = line.split('|')[1:-1]  # å»æ‰é¦–å°¾çš„ |
        processed_cells = []
        
        for cell in cells:
            # æ£€æŸ¥å•å…ƒæ ¼æ˜¯å¦åŒ…å«å›¾ç‰‡
            img_match = re.search(r'!\[(.*?)\]\((.*?)\)', cell)
            if img_match:
                # è¿™æ˜¯ä¸€ä¸ªåŒ…å«å›¾ç‰‡çš„å•å…ƒæ ¼ï¼Œä½¿ç”¨å›¾ç‰‡è·¯å¾„ä½œä¸ºå ä½ç¬¦
                img_path = img_match.group(2)
                processed_cells.append(img_path)
            else:
                # æ™®é€šæ–‡æœ¬å•å…ƒæ ¼
                processed_cells.append(cell.strip())
        
        rows.append(processed_cells)
    
    # åˆ›å»ºè¡¨æ ¼
    if rows:
        # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrame
        df = pd.DataFrame(rows)
        
        # ä½¿ç”¨st.columnsåˆ›å»ºè¡¨æ ¼å¸ƒå±€
        cols = st.columns(len(rows[0]))
        
        # å¡«å……è¡¨æ ¼å†…å®¹
        for col_idx, col in enumerate(cols):
            for row_idx in range(len(rows)):
                cell_content = rows[row_idx][col_idx]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡è·¯å¾„
                if re.match(r'.*\.(png|jpg|jpeg|gif|bmp|webp)', cell_content, re.IGNORECASE):
                    with col:
                        display_image(cell_content, base_dir)
                else:
                    with col:
                        st.markdown(cell_content)

def display_markdown_with_images(markdown_content, base_dir="."):
    """è§£æMarkdownå†…å®¹å¹¶ä½¿ç”¨Streamlitç»„ä»¶æ˜¾ç¤ºï¼ŒåŒ…æ‹¬å›¾ç‰‡å’Œè¡¨æ ¼"""
    # åˆ†å‰²å†…å®¹ä¸ºè¡Œ
    lines = markdown_content.split('\n')
    i = 0
    while i < len(lines):
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼å¼€å§‹
        if lines[i].strip().startswith('|') and '|' in lines[i][1:]:
            # æ”¶é›†è¡¨æ ¼è¡Œ
            table_lines = []
            while i < len(lines) and lines[i].strip().startswith('|'):
                table_lines.append(lines[i])
                i += 1
            
            # å¤„ç†è¡¨æ ¼
            process_markdown_table(table_lines, base_dir)
        else:
            # å¤„ç†æ™®é€šæ–‡æœ¬è¡Œ
            if lines[i].strip():
                process_markdown_line(lines[i], base_dir)
            i += 1

def main():
    st.set_page_config(page_title="æ¯æ—¥arXivè®ºæ–‡å¿«æŠ¥", page_icon="ğŸ“š")
    
    st.title("AI4Paper")
    st.write("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ç”Ÿæˆä»Šæ—¥ç²¾é€‰è®ºæ–‡å¿«æŠ¥")
    
    # æ·»åŠ ä¸€äº›é…ç½®é€‰é¡¹
    with st.expander("é«˜çº§é…ç½®"):
        days_back = st.slider("æŸ¥è¯¢è¿‡å»å‡ å¤©çš„è®ºæ–‡", 1, 7, 3)
        
        # ç›®æ ‡æœºæ„å¤šé€‰
        default_orgs = [
            "Google", "Meta", "Microsoft", "OpenAI", 
            "Anthropic", "DeepMind", "Stanford", 
            "Alibaba", "Huawei", "Baidu",
        ]
        target_orgs = st.multiselect(
            "ç›®æ ‡æœºæ„", 
            options=default_orgs + ["å…¶ä»–"],
            default=default_orgs
        )
        
        # å¦‚æœé€‰æ‹©äº†"å…¶ä»–"ï¼Œå…è®¸ç”¨æˆ·è¾“å…¥è‡ªå®šä¹‰æœºæ„
        if "å…¶ä»–" in target_orgs:
            target_orgs.remove("å…¶ä»–")
            custom_org = st.text_input("è¾“å…¥è‡ªå®šä¹‰æœºæ„åç§°")
            if custom_org:
                target_orgs.append(custom_org)
    
    # ç”ŸæˆæŒ‰é’®
    if st.button("ç”Ÿæˆæ¯æ—¥è®ºæ–‡å¿«æŠ¥", type="primary"):
        with st.spinner("æ­£åœ¨è·å–å’Œå¤„ç†è®ºæ–‡ï¼Œè¯·ç¨å€™..."):
            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = st.progress(0)
            
            # ç¬¬ä¸€æ­¥ï¼šè¿è¡Œpipelineè·å–è®ºæ–‡
            st.info("æ­¥éª¤1/3: ä»arXivè·å–è®ºæ–‡...")
            progress_bar.progress(10)
            
            csv_filename = "papers.csv"
            pdf_folder = "pdf_folder"
            
            # ä½¿ç”¨é»˜è®¤çš„æŸ¥è¯¢å­—ç¬¦ä¸² "cat:cs.AI"
            query = "cat:cs.AI"
            
            papers_count = run_pipeline(
                csv_filename=csv_filename,
                pdf_folder=pdf_folder,
                query=query,
                days_back=days_back,
                target_orgs=target_orgs
            )
            
            if papers_count == 0:
                st.error("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œè¯·è°ƒæ•´æŸ¥è¯¢å‚æ•°åé‡è¯•ã€‚")
                return
            
            progress_bar.progress(40)
            
            # ç¬¬äºŒæ­¥ï¼šè·å–ç›®æ ‡æœºæ„çš„è®ºæ–‡ç´¢å¼•
            st.info("æ­¥éª¤2/3: æ¨¡å‹ç­›é€‰ç›®æ ‡æœºæ„è®ºæ–‡...")
            analyzer = affiliation_analyzer.AffiliationAnalyzer()
            indices_result = analyzer.process_csv(csv_filename, target_orgs)
            
            progress_bar.progress(70)
            
            # ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½è®ºæ–‡å¹¶ç”Ÿæˆæ‘˜è¦
            st.info("æ­¥éª¤3/3: ç”Ÿæˆè®ºæ–‡æ‘˜è¦...")
            assistant = PaperAssistant(output_dir=pdf_folder)
            markdown_content = assistant.process_and_download(csv_filename, indices_result)
            
            
            progress_bar.progress(100)
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"æˆåŠŸç”Ÿæˆè®ºæ–‡å¿«æŠ¥")
            
            # ä½¿ç”¨æ–°çš„display_markdown_with_imageså‡½æ•°æ˜¾ç¤ºmarkdownå†…å®¹
            display_markdown_with_images(markdown_content, pdf_folder)
            
            # æä¾›ä¸‹è½½é“¾æ¥
            st.markdown(
                get_download_link(markdown_content, "æ¯æ—¥arxivç²¾é€‰è®ºæ–‡.md", "ç‚¹å‡»ä¸‹è½½Markdownæ–‡ä»¶"),
                unsafe_allow_html=True
            )

if __name__ == "__main__":
    main() 