import streamlit as st
import datetime as dt
import pandas as pd
from arxiv_pdf import fetch_papers
from paper_affiliation_classifier import PaperAffiliationClassifier
from paper_assistant import PaperAssistant
import affiliation_analyzer
from abstract_matcher import AbstractMatcher
from orgs import orgs
from tools import clean_folder, is_pipeline_running, start_pipeline_background
from output_file_format_manager import (
    get_download_link, get_binary_file_downloader_html, 
    display_markdown_with_images, markdown_to_docx, load_default_markdown
)
import os
def provide_download_links(markdown_content, docx_content, filename_prefix="AIç”Ÿæˆ_æ¯æ—¥arXivç²¾é€‰è®ºæ–‡"):
    """æä¾›markdownå’Œdocxæ ¼å¼çš„ä¸‹è½½é“¾æ¥"""
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(
            get_download_link(markdown_content, f"{filename_prefix}.md", "ç‚¹å‡»ä¸‹è½½Markdownæ–‡ä»¶"),
            unsafe_allow_html=True
        )
    with col2:
        st.markdown(
            get_binary_file_downloader_html(docx_content, "ç‚¹å‡»ä¸‹è½½Wordæ–‡ä»¶", f"{filename_prefix}.md.docx"),
            unsafe_allow_html=True
        )

def main():
    st.set_page_config(page_title="æ¯æ—¥arXivè®ºæ–‡å¿«æŠ¥", page_icon="ğŸ“š")
    
    st.title("AI4Paper")

    if os.path.exists("paper_pipeline.log"):
        print("paper_pipeline.logå­˜åœ¨")
    
    # æ£€æŸ¥å¹¶å¯åŠ¨paper_pipeline.py
    if not is_pipeline_running():
        st.info("æµæ°´çº¿æœªå¯åŠ¨ï¼Œç°åœ¨å¯åŠ¨è®ºæ–‡å¤„ç†æµæ°´çº¿")
        start_pipeline_background(st)
    else:
        st.info("è®ºæ–‡å¤„ç†æµæ°´çº¿æ­£åœ¨åå°è¿è¡Œä¸­")
    
    # åŠ è½½é»˜è®¤çš„markdownæ–‡ä»¶
    default_markdown = load_default_markdown()
    
    
    st.write("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ç”Ÿæˆè‡ªå®šä¹‰è®ºæ–‡å¿«æŠ¥")
    
    # æ·»åŠ ä¸€äº›é…ç½®é€‰é¡¹
    with st.expander("é«˜çº§é…ç½®"):
        days_back = st.slider("æŸ¥è¯¢è¿‡å»å‡ å¤©çš„è®ºæ–‡", 1, 7, 1)
        
        # ç›®æ ‡æœºæ„å¤šé€‰
        default_orgs = orgs
        target_orgs = st.multiselect(
            "ç›®æ ‡æœºæ„", 
            options=default_orgs,
            default=default_orgs
        )
        
        # æ·»åŠ å…³é”®è¯æŸ¥è¯¢é€‰é¡¹
        use_keyword_filter = st.checkbox("ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿‡æ»¤è®ºæ–‡", value=False)
        keyword_query = ""
        if use_keyword_filter:
            keyword_query = st.text_input("è¯·è¾“å…¥æç¤ºè¯", placeholder="ä¾‹å¦‚ï¼šå¼ºåŒ–å­¦ä¹ ã€æœºå™¨äººã€å¤§æ¨¡å‹ç­‰")
        
    
    is_refresh = st.button("ç”Ÿæˆè‡ªå®šä¹‰è®ºæ–‡å¿«æŠ¥", type="primary")
    
    # ç”ŸæˆæŒ‰é’®
    if is_refresh:
        # æ¸…ç©ºpdf_folderå’Œimagesæ–‡ä»¶å¤¹
        clean_folder("pdf_folder")
        clean_folder("images")

        with st.spinner("æ­£åœ¨è·å–å’Œå¤„ç†è®ºæ–‡ï¼Œè¯·ç¨å€™..."):
            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = st.progress(0)
            
            # ç¬¬ä¸€æ­¥ï¼šè¿è¡Œpipelineè·å–è®ºæ–‡,å¹¶æ‰“ä¸Šåˆ†ç±»æ ‡ç­¾
            st.info("æ­¥éª¤1/4: ä»arXivè·å–è®ºæ–‡...")
            progress_bar.progress(10)
            
            csv_filename = "papers.csv"
            pdf_folder = "pdf_folder"
            
            # ä½¿ç”¨é»˜è®¤çš„æŸ¥è¯¢å­—ç¬¦ä¸² "cat:cs.AI"
            query = "cat:cs.AI"
            
            end_date = dt.datetime.today()
            start_date = end_date - dt.timedelta(days=days_back)
            papers_count = fetch_papers(
                pdf_folder, 
                csv_filename=csv_filename,
                query=query,
                author_filter=False,
                start_date=start_date,
                end_date=end_date
            )
            
            if papers_count == 0:
                st.error("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œè¯·è°ƒæ•´æŸ¥è¯¢å‚æ•°åé‡è¯•ã€‚")
                return
            
            classifier = PaperAffiliationClassifier()
            classifier.process_csv(csv_filename)
            
            progress_bar.progress(30)
            
            # ç¬¬äºŒæ­¥ï¼šè·å–ç›®æ ‡æœºæ„çš„è®ºæ–‡ç´¢å¼•
            indices_result = []
            if target_orgs:
                st.info("æ­¥éª¤2/4: æ¨¡å‹ç­›é€‰ç›®æ ‡æœºæ„è®ºæ–‡...")
                analyzer = affiliation_analyzer.AffiliationAnalyzer()
                indices_result = analyzer.process_csv(csv_filename, target_orgs)
                progress_bar.progress(50)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®å…³é”®è¯è¿‡æ»¤è®ºæ–‡
            keyword_indices = []
            if use_keyword_filter and keyword_query:
                st.info("æ­¥éª¤3/4: æ ¹æ®å…³é”®è¯è¿‡æ»¤è®ºæ–‡...")
                matcher = AbstractMatcher()
                keyword_indices = matcher.process_csv(csv_filename, keyword_query)
                progress_bar.progress(70)
            else:
                st.info("æ­¥éª¤3/4: è·³è¿‡å…³é”®è¯è¿‡æ»¤...")
                progress_bar.progress(70)
            
            # åˆå¹¶è¿‡æ»¤ç»“æœ
            final_indices = []
            if target_orgs and use_keyword_filter and keyword_query:
                # ä¸¤ç§è¿‡æ»¤å™¨éƒ½å¯ç”¨ï¼Œå–äº¤é›†
                final_indices = list(set(indices_result).intersection(set(keyword_indices)))
                if not final_indices:
                    st.warning("æ²¡æœ‰åŒæ—¶æ»¡è¶³æœºæ„å’Œå…³é”®è¯æ¡ä»¶çš„è®ºæ–‡ï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰æ»¡è¶³æœºæ„æ¡ä»¶çš„è®ºæ–‡")
                    final_indices = indices_result
            elif target_orgs:
                # åªä½¿ç”¨æœºæ„è¿‡æ»¤
                final_indices = indices_result
            elif use_keyword_filter and keyword_query:
                # åªä½¿ç”¨å…³é”®è¯è¿‡æ»¤
                final_indices = keyword_indices
            else:
                # éƒ½ä¸ä½¿ç”¨ï¼Œè·å–æ‰€æœ‰è®ºæ–‡ç´¢å¼•
                df = pd.read_csv(csv_filename)
                final_indices = list(range(len(df)))
            
            if not final_indices:
                st.error("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œè¯·è°ƒæ•´è¿‡æ»¤æ¡ä»¶åé‡è¯•ã€‚")
                return
            
            # ç¬¬å››æ­¥ï¼šä¸‹è½½è®ºæ–‡å¹¶ç”Ÿæˆæ‘˜è¦
            st.info("æ­¥éª¤4/4: ç”Ÿæˆè®ºæ–‡æ‘˜è¦...")
            assistant = PaperAssistant(output_dir=pdf_folder)
            markdown_content = assistant.process_and_download(csv_filename, final_indices)
            
            progress_bar.progress(100)
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"æˆåŠŸç”Ÿæˆè®ºæ–‡å¿«æŠ¥ï¼Œå…±åŒ…å« {len(final_indices)} ç¯‡è®ºæ–‡ï¼ˆä» {papers_count} ç¯‡è®ºæ–‡ä¸­ç­›é€‰ï¼‰")
            # ä½¿ç”¨æ–°çš„display_markdown_with_imageså‡½æ•°æ˜¾ç¤ºmarkdownå†…å®¹
            display_markdown_with_images(markdown_content)
            
            # ç”ŸæˆWordæ–‡æ¡£
            docx_content = markdown_to_docx(markdown_content)
            
            # æä¾›ä¸‹è½½é“¾æ¥
            provide_download_links(markdown_content, docx_content)
    
    # åªæœ‰åœ¨æ²¡æœ‰ç‚¹å‡»åˆ·æ–°æŒ‰é’®æ—¶æ‰æ˜¾ç¤ºé»˜è®¤markdown
    elif default_markdown:
        st.info("å·²ç”Ÿæˆé»˜è®¤çš„è®ºæ–‡å¿«æŠ¥ï¼ˆæŒ‰æœºæ„ç­›é€‰ï¼Œæ¯12å°æ—¶åˆ·æ–°ä¸€æ¬¡ï¼‰ï¼Œä»¥ä¸‹ä¸ºé¢„åŠ è½½å†…å®¹ï¼š")
        display_markdown_with_images(default_markdown)
        # ç”ŸæˆWordæ–‡æ¡£
        docx_content = markdown_to_docx(default_markdown)
        # æä¾›ä¸‹è½½é“¾æ¥
        provide_download_links(default_markdown, docx_content)

if __name__ == "__main__":
    main() 